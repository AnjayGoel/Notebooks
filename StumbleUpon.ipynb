{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StumbleUpon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1V1er7_oqFipKqTRld65SSnQyaSRG0r_p",
      "authorship_tag": "ABX9TyO1PHeO39SRbGBtT6VDeGUg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejqVAA3EYAtJ"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/datasets/StumbleUpon/test.tsv.zip\"\r\n",
        "!unzip \"/content/drive/MyDrive/datasets/StumbleUpon/train.tsv.zip\"\r\n",
        "#!unzip \"/content/drive/MyDrive/datasets/glove.42B.300d.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVNfbUUIne3V"
      },
      "source": [
        "!pip3 install tldextract\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from random import *\r\n",
        "from scipy.sparse import csr_matrix\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import scipy\r\n",
        "import seaborn as sns\r\n",
        "import tldextract\r\n",
        "import torch\r\n",
        "import json\r\n",
        "import torch.nn as nn\r\n",
        "from torch.optim import SGD\r\n",
        "from torch.nn import Linear,Sigmoid\r\n",
        "import torch.utils.data as D\r\n",
        "import torchsummary\r\n",
        "import torchtext\r\n",
        "nltk.download('wordnet')\r\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwzPJct1Yz3l"
      },
      "source": [
        "stopwords = []\r\n",
        "f = open(\"/content/drive/MyDrive/datasets/stopwords.txt\",\"r\")\r\n",
        "for word in f.readlines():\r\n",
        "  stopwords.append(word.strip())"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6xruBliouFO"
      },
      "source": [
        "#confusion matrix\r\n",
        "def plot_cm(y_test, y_pred):\r\n",
        "  cf_matrix = metrics.confusion_matrix(y_test, y_pred)\r\n",
        "  group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\r\n",
        "  group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\r\n",
        "  group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\r\n",
        "  labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\r\n",
        "  labels = np.asarray(labels).reshape(2,2)\r\n",
        "  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MEVsgOrnqjl"
      },
      "source": [
        "#replace category with relative frequency\r\n",
        "def replace_word(df,col_names):\r\n",
        "  for col_name in col_names:\r\n",
        "    rel = {}\r\n",
        "    for i in df[col_name].unique():\r\n",
        "      df_t = df[df[col_name]==i]\r\n",
        "      rel[i] = len(df_t[df_t[\"label\"]==1])/len(df_t)\r\n",
        "    df[col_name] = df[col_name].apply(lambda x:rel[x]).astype('float64')\r\n",
        "\r\n",
        "#preprocessing\r\n",
        "def get_df(name,sep='\\t'):\r\n",
        "  df = pd.read_csv(name,sep='\\t')\r\n",
        "  domainExt = lambda x: pd.Series([tldextract.extract(x[\"url\"]).domain,tldextract.extract(x[\"url\"]).suffix])\r\n",
        "  df[['domain', 'suffix']] = df.apply(domainExt, axis=1)\r\n",
        "  textExt = lambda x: pd.Series([json.loads(x[\"boilerplate\"]).get(\"url\"),json.loads(x[\"boilerplate\"]).get(\"title\"),json.loads(x[\"boilerplate\"]).get(\"body\")])\r\n",
        "  df[['urlText','title', 'body']] = df.apply(textExt, axis=1)\r\n",
        "  df.fillna(\"\",inplace=True)\r\n",
        "  df[\"urlText\"] = df[\"urlText\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(z) for z in simple_preprocess(x)]))\r\n",
        "  df[\"title\"] = df[\"title\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(z) for z in simple_preprocess(x)]))\r\n",
        "  df[\"body\"] = df[\"body\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(z) for z in simple_preprocess(x)]))\r\n",
        "  df['text'] = df.apply(lambda x:\" \".join([x['urlText'],x['urlText'],x['title'],x['title'],x['title'],x['body']]),axis=1)\r\n",
        "  df[\"alchemy_category_score\"]= df[\"alchemy_category_score\"].replace('?',0.5).astype('float64')\r\n",
        "  df[\"is_news\"]= df[\"is_news\"].replace('?',0).astype('int')\r\n",
        "  df[\"news_front_page\"]= df[\"news_front_page\"].replace('?',0).astype('int')\r\n",
        "  df[\"alchemy_category\"]= df[\"alchemy_category\"].replace('?',\"unknown\")\r\n",
        "  df.drop(columns=[\"url\", \"urlid\",\"boilerplate\",\"framebased\"],inplace=True)\r\n",
        "  #replace_word(df,[\"suffix\",\"domain\",\"alchemy_category\"])\r\n",
        "  return df\r\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4axJFRzl5hf"
      },
      "source": [
        "#for creating imbalanced test set set unequal values.\r\n",
        "split_pos = 0.1 #test size for positive\r\n",
        "split_neg = 0.1 #test size for negative\r\n",
        "\r\n",
        "df_main = get_df(\"./train.tsv\")\r\n",
        "df_pos = df_main[df_main[\"label\"]==1]\r\n",
        "df_neg = df_main[df_main[\"label\"]==0]\r\n",
        "\r\n",
        "df_pos_train,df_pos_test = train_test_split(df_pos, test_size=split_pos)\r\n",
        "df_neg_train,df_neg_test = train_test_split(df_neg, test_size=split_neg)\r\n",
        "\r\n",
        "df_train = pd.concat([df_pos_train, df_neg_train], ignore_index=True)\r\n",
        "df_test = pd.concat([df_pos_test, df_neg_test], ignore_index=True)\r\n",
        "\r\n",
        "del df_pos,df_neg,df_pos_train,df_neg_train,df_pos_test,df_neg_test"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dJ0zPSaj7u-"
      },
      "source": [
        "#tfidf vectors used in logistic regression.\r\n",
        "urlTFIDF = TfidfVectorizer(use_idf=True,min_df = 5)\r\n",
        "urlTFIDF.fit(df_main[\"urlText\"])\r\n",
        "titleTFIDF = TfidfVectorizer(use_idf=True,min_df = 5)\r\n",
        "titleTFIDF.fit(df_main[\"title\"])\r\n",
        "bodyTFIDF = TfidfVectorizer(use_idf=True,min_df = 5)\r\n",
        "bodyTFIDF.fit(df_main[\"body\"])\r\n",
        "\r\n",
        "pipe = Pipeline(steps=[('scale', StandardScaler()),('pca', PCA(n_components=0.80))])\r\n",
        "pipe.fit(df_main.drop(columns=[\"label\"]).select_dtypes('number').to_numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmkT97vJJKR9"
      },
      "source": [
        "def get_data(df):\r\n",
        "  x1 = pipe.transform(df.drop(columns=[\"label\"]).select_dtypes('number').to_numpy())\r\n",
        "  x2 = urlTFIDF.transform(df[\"urlText\"])\r\n",
        "  x3 = titleTFIDF.transform(df[\"title\"])\r\n",
        "  x4 = bodyTFIDF.transform(df[\"body\"])\r\n",
        "  x5 = pd.get_dummies(df_main.alchemy_category, prefix='category',dtype='float').to_numpy()\r\n",
        "  y = df[\"label\"].to_numpy()\r\n",
        "  return x1,x2,x3,x4,x5,y"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_SuWAlg19fm"
      },
      "source": [
        "def pcaPlot(x):\r\n",
        "  pca = PCA()\r\n",
        "  pca.fit(x)\r\n",
        "  cumsum = np.cumsum(pca.explained_variance_ratio_)*100\r\n",
        "  d = [n for n in range(len(cumsum))]\r\n",
        "  plt.figure(figsize=(10, 10))\r\n",
        "  plt.plot(d,cumsum, color = 'red',label='cumulative explained variance')\r\n",
        "  plt.title('Cumulative Explained Variance as a Function of the Number of Components')\r\n",
        "  plt.ylabel('Cumulative Explained variance')\r\n",
        "  plt.xlabel('Principal components')\r\n",
        "  plt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\r\n",
        "  plt.legend(loc='best')\r\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_WH8T9R74LL"
      },
      "source": [
        "#sparse matrix to pytorch tensor\r\n",
        "def csrToTorch(x):\r\n",
        "  coo = x.tocoo()\r\n",
        "  values = coo.data\r\n",
        "  indices = np.vstack((coo.row, coo.col))\r\n",
        "  i = torch.LongTensor(indices)\r\n",
        "  v = torch.FloatTensor(values)\r\n",
        "  shape = coo.shape\r\n",
        "  #print(shape)\r\n",
        "  return torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsZJY0ld-EQ1"
      },
      "source": [
        "device = 'cpu'\r\n",
        "class LogisticLayer(nn.Module):\r\n",
        "    def __init__(self, n_inputs):\r\n",
        "        super(LogisticLayer, self).__init__()\r\n",
        "        self.hidden1 = Linear(n_inputs, 1)\r\n",
        "        #self.hidden2 = Linear(3, 1)\r\n",
        "        self.act = Sigmoid()\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        X = self.hidden1(X)\r\n",
        "        #X = self.hidden2(X)\r\n",
        "        X = self.act(X)\r\n",
        "        return X\r\n",
        "\r\n",
        "class EnsembleLogistics(nn.Module):\r\n",
        "    def __init__(self,input_x1,input_x2,input_x3,input_x4,input_x5):\r\n",
        "        super(EnsembleLogistics, self).__init__()\r\n",
        "\r\n",
        "        self.m1 = LogisticLayer(input_x1)\r\n",
        "        self.m2 = LogisticLayer(input_x2)\r\n",
        "        self.m3 = LogisticLayer(input_x3)\r\n",
        "        self.m4 = LogisticLayer(input_x4)\r\n",
        "        self.m5 = LogisticLayer(input_x5)\r\n",
        "        self.m0 = LogisticLayer(5)\r\n",
        "\r\n",
        "        \r\n",
        "    def forward(self, x1, x2, x3,x4,x5):\r\n",
        "        x1 = torch.flatten(self.m1(x1))\r\n",
        "        x2 = torch.flatten(self.m2(x2))\r\n",
        "        x3 = torch.flatten(self.m3(x3))\r\n",
        "        x4 = torch.flatten(self.m4(x4))\r\n",
        "        x5 = torch.flatten(self.m5(x5))#.reshape(-1,1)\r\n",
        "        x = torch.stack((x1, x2, x3,x4,x5),1)\r\n",
        "        #print(x)\r\n",
        "        #print(x5)\r\n",
        "        #x = x5*x\r\n",
        "        #print(x)\r\n",
        "        x = torch.flatten(self.m0(x))\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class EnsembleDatasets(D.Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, x1,x2_csr,x3_csr,x4_csr,x5, label, device='cpu'):\r\n",
        "        self.device = torch.device(device)\r\n",
        "        self.x1 = torch.tensor(x1,dtype = torch.float32)\r\n",
        "        self.x2_csr = x2_csr\r\n",
        "        self.x3_csr = x3_csr\r\n",
        "        self.x4_csr = x4_csr\r\n",
        "        self.x5 = torch.tensor(x5,dtype = torch.float32)\r\n",
        "        self.label = torch.tensor(label, dtype = torch.float32, device=self.device)\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return self.label.shape[0]\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        x1 = self.x1[index]\r\n",
        "        x2 = csrToTorch(self.x2_csr[index])\r\n",
        "        x3 = csrToTorch(self.x3_csr[index])\r\n",
        "        x4 = csrToTorch(self.x4_csr[index])\r\n",
        "        x5 = self.x5[index]\r\n",
        "        return x1,x2,x3,x4,x5,self.label[index]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYfUIgjOIIMa"
      },
      "source": [
        "def train_model(model,x1,x2_csr,x3_csr,x4_csr,x5, label,batchsize,epochs):\r\n",
        "    data_tr = EnsembleDatasets(x1,x2_csr,x3_csr,x4_csr,x5,label, device)\r\n",
        "    criterion = nn.BCELoss()\r\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
        "    train_ldr = D.DataLoader(dataset=data_tr, batch_size=batchsize, shuffle=True)\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "      print(f\"epoch: {epoch}\")\r\n",
        "      for x1,x2,x3,x4,x5,label in train_ldr:\r\n",
        "        optimizer.zero_grad()\r\n",
        "        yhat = model(x1,x2,x3,x4,x5)\r\n",
        "        loss = criterion(yhat,label)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "      #yhat = (yhat>0.5).float()\r\n",
        "      #correct = (yhat == label).float().sum()\r\n",
        "      #print(\"Accuracy: {:.3f}\".format(correct/label.shape[0]))\r\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjfHmCfNLdgU"
      },
      "source": [
        "x1,x2,x3,x4,x5,label = get_data(df_train)\r\n",
        "model = EnsembleLogistics(x1.shape[1],x2.shape[1],x3.shape[1],x4.shape[1],x5.shape[1])\r\n",
        "train_model(model,x1,x2,x3,x4,x5,label,5,20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAGSJITeLxI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa86820-6de2-4ac6-9875-0b09c4ad3766"
      },
      "source": [
        "#test\r\n",
        "x1,x2,x3,x4,x5,label = get_data(df_test)\r\n",
        "data_ldr  = D.DataLoader(dataset=EnsembleDatasets(x1,x2,x3,x4,x5,label, 'cpu'),batch_size=len(df_train))\r\n",
        "for x1,x2,x3,x4,x5,label in data_ldr:\r\n",
        "      yhat = model(x1,x2,x3,x4,x5)\r\n",
        "      yhat = (yhat>0.5).float()\r\n",
        "      correct = (yhat == label).float().sum()\r\n",
        "      print(yhat.shape[0])\r\n",
        "      print(\"Accuracy: {:.3f}\".format(correct/label.shape[0]))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "740\n",
            "Accuracy: 0.818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAMNIFch-tHs"
      },
      "source": [
        "--------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ihui-Q7rZcd"
      },
      "source": [
        "wordToIdx= {}\r\n",
        "embedding_matrix = torch.zeros(size=(1917495,300))\r\n",
        "i = 0\r\n",
        "\r\n",
        "with open(\"./glove.42B.300d.txt\", 'r') as f:\r\n",
        "    for line in f:\r\n",
        "        values = line.split(' ')\r\n",
        "        word = values[0]\r\n",
        "        vector = torch.from_numpy(np.asarray(values[1:], \"float32\"))\r\n",
        "        embedding_matrix[i]=vector\r\n",
        "        wordToIdx[word] = i\r\n",
        "        i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf8HMV15sGJx"
      },
      "source": [
        "embedding = nn.Embedding.from_pretrained(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM0F5b_Jxtf4"
      },
      "source": [
        "input = torch.LongTensor([1000])\r\n",
        "embedding(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_RQdmDUwEt_"
      },
      "source": [
        "list(embeddings.keys())[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6lPDCKTOxYC"
      },
      "source": [
        "class tfidf:\r\n",
        "  def __init__(self):\r\n",
        "    self.tfidf_vectorizer=TfidfVectorizer(use_idf=True,min_df=3, max_features=None, strip_accents='unicode',  \r\n",
        "                        analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), \r\n",
        "                        smooth_idf=1,sublinear_tf=1,\r\n",
        "                        #stop_words=stopwords.words('english'),\r\n",
        "                        )\r\n",
        "\r\n",
        "  def fit(self,docs):\r\n",
        "    self.tfidf_vectorizer.fit(docs)\r\n",
        "    self.vocab = self.tfidf_vectorizer.get_feature_names()\r\n",
        "  def get_keywords(self,docs,count):\r\n",
        "    tfidf_vectorizer_vectors = self.tfidf_vectorizer.transform(docs)\r\n",
        "    keywords = []\r\n",
        "    for vec in tfidf_vectorizer_vectors:\r\n",
        "      kws = []\r\n",
        "      for i,score in enumerate(list(np.squeeze(np.asarray(vec.T.todense())))):\r\n",
        "        if(score>0.0 and self.vocab[i] not in stopwords):\r\n",
        "          kws.append([score,self.vocab[i]])\r\n",
        "      kws.sort(key = lambda x: x[0],reverse=True) \r\n",
        "      keywords.append([x[1] for x in kws[:count]])\r\n",
        "    return keywords"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}